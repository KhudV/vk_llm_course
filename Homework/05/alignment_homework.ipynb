{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"0033ac842aab47e48f35151855112979":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"015524419b8a4a21bec9ca2462a1bdae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3bb5755a3f44b519c4203430846e3d2","max":390,"min":0,"orientation":"horizontal","style":"IPY_MODEL_62e010f8c15d402aba1e045de8e8a50b","value":390}},"0970efe0166f40b4a4355f474bfe182a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_305a6c274faa46beb46e8a2eff32d0fe","placeholder":"​","style":"IPY_MODEL_5eec4a26a59c4b108bf576f21cad07f0","value":" 24895/24895 [00:31&lt;00:00, 819.84 examples/s]"}},"0daa5834f1834dea9d5e40e6a3c4b376":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11b0022cfbaa488db010a2f22a17b7cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fd865cd993242cdad016d9794a75f6e","placeholder":"​","style":"IPY_MODEL_f986e194b4944ead94190797f28015fe","value":""}},"11f6a941d42f4692a53f94f36aca62d5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14bab8c0ec8f4b7ea41d4e74a31d280b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c6d1254f73fc441897fb289661579fcc","IPY_MODEL_4cdf531ea3f545ffa6e0d0f68389f653","IPY_MODEL_a36ee9303657431093a8b33d713e9bd8"],"layout":"IPY_MODEL_daad181cf83044a5bc285f7925176464"}},"305a6c274faa46beb46e8a2eff32d0fe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"351c280e3e0644e08478893632ca878c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_579edffceded4184ab0009cef5e8d87e","placeholder":"​","style":"IPY_MODEL_83eac7e3fc554ea09c2a41452cad2cde","value":" 391/? [02:18&lt;00:00,  3.09it/s]"}},"3fd865cd993242cdad016d9794a75f6e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cdf531ea3f545ffa6e0d0f68389f653":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f67d8a3f95074b01853851dd5b41cfa6","max":25000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b4b6c04b33e44613a3736505a4899d71","value":25000}},"4ddb8f654c5a45e38c588cac213d0eb8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5470a457a3e2466789ab2f3ca3289daa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_afb680bcb0fc4f4a94f04b5c086f77c6","IPY_MODEL_e76aa419c1864ab7906f58c09b53efb8","IPY_MODEL_0970efe0166f40b4a4355f474bfe182a"],"layout":"IPY_MODEL_64858c8a884d4e0792e0f6894015ead5"}},"5482e18aa0564a50a7599875aa7e053f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"579edffceded4184ab0009cef5e8d87e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5dab60bfa0f54cadbc163b4331732185":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5eec4a26a59c4b108bf576f21cad07f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"60cbfb4df6da47d4b8b9624047b91cf2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62e010f8c15d402aba1e045de8e8a50b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"64858c8a884d4e0792e0f6894015ead5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d693232f3c048ad944a47ee5742187c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"73db6e451d7c401d8b341c4352809e20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b5922a62de94ee29cb6eefc9193190d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_60cbfb4df6da47d4b8b9624047b91cf2","placeholder":"​","style":"IPY_MODEL_73db6e451d7c401d8b341c4352809e20","value":" 30/50 [25:39&lt;15:10, 45.54s/it]"}},"83eac7e3fc554ea09c2a41452cad2cde":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"90b8fdb8b9fb438f84c4f390683b0f3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_af3a05a0c4ca49a385776d6c42427f8a","IPY_MODEL_ce8dea39d6b44be18e8f94392f94e7ce","IPY_MODEL_7b5922a62de94ee29cb6eefc9193190d"],"layout":"IPY_MODEL_0daa5834f1834dea9d5e40e6a3c4b376"}},"9123a18088fb454d915218f37f72a790":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0134acdd9764db08cdb145a3b816eb3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a35d7bf116124060b525d12cd119ac5b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a36ee9303657431093a8b33d713e9bd8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f49a71e7d53342809dc887e254aa9d9f","placeholder":"​","style":"IPY_MODEL_c52d8a32faa54cd9b7541441d3551ded","value":" 25000/25000 [00:00&lt;00:00, 9679.51 examples/s]"}},"af3a05a0c4ca49a385776d6c42427f8a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_11f6a941d42f4692a53f94f36aca62d5","placeholder":"​","style":"IPY_MODEL_6d693232f3c048ad944a47ee5742187c","value":" 60%"}},"afb680bcb0fc4f4a94f04b5c086f77c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0134acdd9764db08cdb145a3b816eb3","placeholder":"​","style":"IPY_MODEL_b07ef37f928646c7807471849cf6f71a","value":"Map: 100%"}},"b07ef37f928646c7807471849cf6f71a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4b6c04b33e44613a3736505a4899d71":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c3bb5755a3f44b519c4203430846e3d2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c52d8a32faa54cd9b7541441d3551ded":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c6d1254f73fc441897fb289661579fcc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5482e18aa0564a50a7599875aa7e053f","placeholder":"​","style":"IPY_MODEL_4ddb8f654c5a45e38c588cac213d0eb8","value":"Filter: 100%"}},"ce8dea39d6b44be18e8f94392f94e7ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_d833f6bf46c04b64acd1c04165c8f813","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0033ac842aab47e48f35151855112979","value":30}},"d833f6bf46c04b64acd1c04165c8f813":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"daad181cf83044a5bc285f7925176464":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e76aa419c1864ab7906f58c09b53efb8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5dab60bfa0f54cadbc163b4331732185","max":24895,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a35d7bf116124060b525d12cd119ac5b","value":24895}},"f49a71e7d53342809dc887e254aa9d9f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f67d8a3f95074b01853851dd5b41cfa6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f70928bb66b24415ad03f49633775e64":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_11b0022cfbaa488db010a2f22a17b7cc","IPY_MODEL_015524419b8a4a21bec9ca2462a1bdae","IPY_MODEL_351c280e3e0644e08478893632ca878c"],"layout":"IPY_MODEL_9123a18088fb454d915218f37f72a790"}},"f986e194b4944ead94190797f28015fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<font color=red>**Danger zone:**</font> you'll be fine-tuning a model to generate positive, negative or even toxic reviews. We'll be doing this for fun, but this is also the technique for [review bombing](https://en.wikipedia.org/wiki/Review_bomb), bot farms on social media and other less than dignified stuff. It is ultimately your decision how you apply this knowledge, but before you choose, ask yourself: is this why you chose to learn ML?\n\n\n# LLMs Alignment with Reinforcement Learning from human feedback (RLHF).\n\n","metadata":{"id":"NvhPa7a59AIG"}},{"cell_type":"markdown","source":"In this homework, you're gonna fine-tune a language model with reinforcement learning to make it generate bad (or good) reviews.\n\nTo perform RL-based fine-tuning, we'll use a new (in this course) library called [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl). TRL implements the main reinforcement learning components of RLHF: reward modeling and fine-tuning with PPO.\n\n![img](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/TRL-readme.png)","metadata":{"id":"bgfL4bSSAXan"}},{"cell_type":"markdown","source":"## Stage 0: load model\n\nTo see how TRL works, we'll use it to align GPT2 on IMDB dataset to generate negative movie reviews. In fact, __it's your choice whether you want positive or negative reviews__, however I recommend you to focus on negative ones, in order to see greater effect after RLHF\n\nBut before you choose, let's take a look at the baseline model: a GPT-2 fine-tuned on generating arbitrary movie reviews.","metadata":{"id":"8cJfrTbFYAx8"}},{"cell_type":"code","source":"!pip install trl peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:16:01.819282Z","iopub.execute_input":"2024-12-11T13:16:01.819570Z","iopub.status.idle":"2024-12-11T13:16:12.562770Z","shell.execute_reply.started":"2024-12-11T13:16:01.819543Z","shell.execute_reply":"2024-12-11T13:16:12.561940Z"}},"outputs":[{"name":"stdout","text":"Collecting trl\n  Downloading trl-0.12.2-py3-none-any.whl.metadata (11 kB)\nCollecting peft\n  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: accelerate>=0.34.0 in /opt/conda/lib/python3.10/site-packages (from trl) (1.1.1)\nRequirement already satisfied: datasets>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from trl) (3.1.0)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from trl) (13.7.1)\nRequirement already satisfied: transformers<4.47.0 in /opt/conda/lib/python3.10/site-packages (from trl) (4.46.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.26.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (2.32.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.21.0->trl) (2024.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.9.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<4.47.0->trl) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<4.47.0->trl) (0.20.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (2.18.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (4.0.3)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2024.6.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.16.0)\nDownloading trl-0.12.2-py3-none-any.whl (365 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: trl, peft\nSuccessfully installed peft-0.14.0 trl-0.12.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport transformers\nimport datasets\nimport trl\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmain_tokenizer = transformers.AutoTokenizer.from_pretrained(\"lvwerra/gpt2-imdb\")\nmain_model = transformers.AutoModelForCausalLM.from_pretrained(\"lvwerra/gpt2-imdb\", device_map=device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pHs22MXdPify","outputId":"a95af581-560b-4845-e202-a84150940196","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:16:12.564651Z","iopub.execute_input":"2024-12-11T13:16:12.564931Z","iopub.status.idle":"2024-12-11T13:16:36.323520Z","shell.execute_reply.started":"2024-12-11T13:16:12.564904Z","shell.execute_reply":"2024-12-11T13:16:36.322825Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/17.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"809521704cfb49d0b11fba4102ec7dd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/577 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82b09b80b72d4ec0b43ed900f7fcb0ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ab0cfd8512044e1a652e0857a210add"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb7a47962ab14709a413f9a9e6d9d3c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f02fffad18274ad3bf6c100fcb3fd991"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"288ec405e5114696a278deb5ad4afa3c"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"!export PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:16:36.324511Z","iopub.execute_input":"2024-12-11T13:16:36.324980Z","iopub.status.idle":"2024-12-11T13:16:37.353247Z","shell.execute_reply.started":"2024-12-11T13:16:36.324952Z","shell.execute_reply":"2024-12-11T13:16:37.352229Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"inputs = main_tokenizer(\"The movie\", return_tensors='pt').to(device)\ngenerated_ids = main_model.generate(**inputs, max_new_tokens=50, do_sample=True)\nprint(\"\\nGenerated text:\", main_tokenizer.decode(generated_ids.flatten().cpu().numpy().tolist()))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KE3jo7uhQrvK","outputId":"b937058f-6014-4180-f5e5-b09ed4a2285a","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:16:37.354815Z","iopub.execute_input":"2024-12-11T13:16:37.355180Z","iopub.status.idle":"2024-12-11T13:16:38.931347Z","shell.execute_reply.started":"2024-12-11T13:16:37.355135Z","shell.execute_reply":"2024-12-11T13:16:38.930238Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nGenerated text: The movie, in my opinion, had no story, no plot, no characters, even no plot. It's absolutely pathetic, the acting was very amateurish. I mean how can a movie like this ever be a success? Even if you enjoyed your time\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"If you run this cell a couple of times, you'll see that the model generates both positive, negative and neutral reviews in some proportion. What we're gonna do next is teach the model to generate more positive (or negative) reviews.\n\nSimilarly to InstructGPT, we're gonna do that in 2 stages:\n- **train a reward model** to assign higher values to positive (or negative) reviews\n- fine-tune the language model to **maximize that reward using [proximal policy optimization](https://openai.com/research/openai-baselines-ppo)**\n\n","metadata":{"id":"dJbfhMEpR4Sz"}},{"cell_type":"markdown","source":"## Stage 1: train a reward model\n\nFirst, we'll train a BERT-like model as our reward model. We'll generate a synthetic pairwise rankings to emulate human rankings.\n\n__Q:__ why do I need a reward model? Can I just use a pre-trained sentiment classifier? <br> __A:__ Yes, you can - but that only works for movie reviews. But this homework will teach you how to do RLHF for any kind objective.\n\n","metadata":{"id":"Bcv4uC7xb26Z"}},{"cell_type":"code","source":"# We'll be fine-tuning a small BERT-like model for now. Please try other models for the main assignment.\nreward_model = transformers.AutoModelForSequenceClassification.from_pretrained(\"allenai/longformer-base-4096\", device_map=device)\nreward_tokenizer = transformers.AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WeOdZ_ayc9dy","outputId":"3813d14b-d113-4154-a237-04bf3d5bbbe5","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:16:38.934199Z","iopub.execute_input":"2024-12-11T13:16:38.934804Z","iopub.status.idle":"2024-12-11T13:16:45.690926Z","shell.execute_reply.started":"2024-12-11T13:16:38.934774Z","shell.execute_reply":"2024-12-11T13:16:45.689957Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2087e8ff76a241db9e76909c5f8727ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb771d3cf2e545ff9e860eafc44e5ea9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/597M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21b7b34e3ab74f06a5b98f3eb8076b4a"}},"metadata":{}},{"name":"stderr","text":"Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c90ecaa224ce47a6a27db8d9cf983541"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24f5b6f863f14ac19ae15b5976e55b47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ffb50b5b5c848398a7cc521cd548aa7"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"__Note that__ the reward model has a separate tokenizer, different from the main model. They don't need to be the same for RLHF fine-tuning.","metadata":{"id":"_ZUUNQo-d11b"}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass IMDBPairwiseDataset(Dataset):\n    \"\"\" \n    A dataset of all possible pairs of chosen and rejected texts for TRL reward training format.\n\n    This dataset is designed to facilitate the training of a reward model by providing pairs of\n    texts where one is preferred (chosen) and the other is not (rejected). Each sample in the dataset\n    is a dictionary containing tokenized input IDs and attention masks for both the chosen and rejected\n    texts.\n\n    Parameters:\n    imdb: dataset to pairwise\n    tokenizer: The tokenizer used to preprocess the texts\n    accepted_label (int): The label that indicates a chosen text. Texts with this label are considered\n                          preferred, while others are considered rejected.\n\n    Methods:\n    __len__(): Returns the total number of possible pairs of chosen and rejected texts.\n    __getitem__(index): Returns a dictionary containing tokenized inputs for a specific pair of chosen\n                        and rejected texts.\n    \"\"\"\n    \n    def __init__(self, imdb, tokenizer, accepted_label):\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.chosen_texts = [sample['text'] for sample in imdb if sample['label'] == accepted_label]\n        self.rejected_texts = [sample['text'] for sample in imdb if sample['label'] != accepted_label]\n\n        assert self.chosen_texts, f\"no texts with label {accepted_label}\"\n        # print(f\"Found {len(self.chosen_texts)} chosen and {len(self.rejected_texts)} rejected texts, {len(self)} pairs\")\n\n        self.column_names = [\n            'input_ids_chosen', 'attention_mask_chosen',\n            'input_ids_rejected', 'attention_mask_rejected'\n        ]\n\n    def __len__(self):\n        return len(self.chosen_texts) * len(self.rejected_texts)\n\n    def __getitem__(self, index: int):\n        i = index // len(self.rejected_texts)\n        j = index % len(self.rejected_texts)\n        chosen_text, rejected_text = self.chosen_texts[i], self.rejected_texts[j]\n        \n        chosen_encodings = self.tokenizer(chosen_text, padding=False, return_tensors=\"pt\")\n        rejected_encodings = self.tokenizer(rejected_text, padding=False, return_tensors=\"pt\")\n        return dict(\n            input_ids_chosen=chosen_encodings['input_ids'].squeeze(0),\n            attention_mask_chosen=chosen_encodings['attention_mask'].squeeze(0),\n            input_ids_rejected=rejected_encodings['input_ids'].squeeze(0),\n            attention_mask_rejected=rejected_encodings['attention_mask'].squeeze(0),\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:16:45.692776Z","iopub.execute_input":"2024-12-11T13:16:45.693184Z","iopub.status.idle":"2024-12-11T13:16:45.703239Z","shell.execute_reply.started":"2024-12-11T13:16:45.693119Z","shell.execute_reply":"2024-12-11T13:16:45.702091Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"TARGET_LABEL = 0 # negative reviews\nimdb = datasets.load_dataset(\"imdb\", split='train')\nreward_data = IMDBPairwiseDataset(imdb, reward_tokenizer, accepted_label=TARGET_LABEL)\n\nsample = reward_data[31337]\nprint('CHOSEN:', reward_tokenizer.decode(sample['input_ids_chosen']))\nprint('REJECTED:', reward_tokenizer.decode(sample['input_ids_rejected']))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"olo-bvgNcwEC","outputId":"21b2a6f2-03cf-47a8-ffd0-c43a85ca331e","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:16:45.704489Z","iopub.execute_input":"2024-12-11T13:16:45.704743Z","iopub.status.idle":"2024-12-11T13:16:52.895080Z","shell.execute_reply.started":"2024-12-11T13:16:45.704718Z","shell.execute_reply":"2024-12-11T13:16:52.894124Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7b5ce5f356d4f3eb60431e7e07d040d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/597M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0697a0f68c3c4b29bb4c4afdd6092a19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2fb653fc7ac46a1a41b63536a92116b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"390f3313c0254a98b92ad6c70e7d8fb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07a02bb20f284399bc9f0753e9efcc67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdf06fee1d194e67bfaeeb5a86742873"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e805ba24b99340148b0dd587da61d624"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c48b93ad724848f28f8b7198b6002026"}},"metadata":{}},{"name":"stdout","text":"CHOSEN: <s>If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br /></s>\nREJECTED: <s>This movie has some things that are pretty amazing. First, it is supposed to be based on a true story. That, in itself, is amazing that multiple tornadoes would hit the same town at night in the fall-in Nebraska. I wonder if the real town's name was close to \"Blainsworth\" (which is the town's name in the movie). There is an Ainsworth, Nebraska, but there is also a town that starts with Blains-something.<br /><br />It does show the slowest moving tornadoes on record in the the seen where the boys are in the house. On the other hand, the scene where the TV goes fuzzy is based in fact. Before Doppler radar and weather radio, we were taught that if you turned your TV to a particular channel (not on cable) and tuned the brightness just right, you could tell if there was a tornado coming. The problem was that by then you would be able to hear it. <br /><br />Since I know something about midwest tornadoes, it made this movie fun for me. I enjoy it more than Twister. I mean, give me a break-there is no way you could make it through and F5 by chaining yourself to a pipe in a well house.</s>\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"We'll be using `trl.RewardTrainer` - a special case of `transformers.Trainer`.\n\n![img](https://i.imgur.com/2JzNAPs.png)\n\nNote that the model itself does not score pairs: it processes chosen ($y_w$) and rejected ($y_l$) samples independently. To minimize this loss, the reward model needs to score chosen sample higher than the rejected one. Note that the formula also assumes some context $x$, which is useful for seq2seq tasks. In our case of movie reviews, $x$ is empty.","metadata":{"id":"kZRczyofiSl0"}},{"cell_type":"code","source":"from peft import LoraConfig\n\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    bias=\"none\",\n    target_modules=[\"query\", \"key\", \"value\", \"query_global\", \"key_global\", \"value_global\", \"dense\"],\n    task_type=\"SEQ_CLS\"  # Sequence Classification\n)\n\n\nnew_max_position_embeddings = 890\n\nreward_model.config.max_position_embeddings = new_max_position_embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:16:52.896484Z","iopub.execute_input":"2024-12-11T13:16:52.896741Z","iopub.status.idle":"2024-12-11T13:16:52.968523Z","shell.execute_reply.started":"2024-12-11T13:16:52.896717Z","shell.execute_reply":"2024-12-11T13:16:52.967888Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"reward_tokenizer.model_max_length = 890","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:16:52.969489Z","iopub.execute_input":"2024-12-11T13:16:52.969760Z","iopub.status.idle":"2024-12-11T13:16:53.046476Z","shell.execute_reply.started":"2024-12-11T13:16:52.969736Z","shell.execute_reply":"2024-12-11T13:16:53.045611Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"training_args = trl.RewardConfig(  # like transformers.TrainingArguments\n    output_dir=\"reward_model\",\n    per_device_train_batch_size=16,\n    gradient_accumulation_steps=1,\n    learning_rate=1.41e-5,\n    max_steps=1_000,              # note: training may need more than 1k steps\n    logging_steps=50,\n    gradient_checkpointing=True,  # reduce memory usage but train ~30% slower\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    fp16=True,                    # disable this on CPU or on very old GPUs\n    report_to='none',\n    # you may add any other hyperparameters that you found useful\n)\n\ntrainer = trl.RewardTrainer(\n    model=reward_model,\n    args=training_args,\n    tokenizer=reward_tokenizer,\n    train_dataset=reward_data,\n    peft_config=peft_config,  # optionally, you may tune with LoRA, prompt-tuning, etc\n)\n\ntrainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"oaQ_-JAzakJs","outputId":"923b8692-5627-43bb-89b4-a42a7d0ff805","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:17:35.634452Z","iopub.execute_input":"2024-12-11T13:17:35.635339Z","iopub.status.idle":"2024-12-11T16:36:22.413862Z","shell.execute_reply.started":"2024-12-11T13:17:35.635303Z","shell.execute_reply":"2024-12-11T16:36:22.412950Z"}},"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\nToken indices sequence length is longer than the specified maximum sequence length for this model (1403 > 890). Running this sequence through the model will result in indexing errors\nYou're using a LongformerTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nInitializing global attention on CLS token...\nInput ids are automatically padded to be a multiple of `config.attention_window`: 512\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\nCould not estimate the number of tokens of the input, floating-point operations will not be computed\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 3:18:07, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>0.693700</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.687900</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.598800</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.177000</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.100400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.085900</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.078100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.082300</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.059100</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.085700</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.061500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.073200</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.066300</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.048700</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.050100</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.063000</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.055200</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.051800</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.071200</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.043800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=0.16167684149742126, metrics={'train_runtime': 11922.4015, 'train_samples_per_second': 1.342, 'train_steps_per_second': 0.084, 'total_flos': 0.0, 'train_loss': 0.16167684149742126, 'epoch': 0.0001024})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"reward_model.gradient_checkpointing_disable()\nreward_model.eval()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CRk7z-2r4C-A","outputId":"29e3b246-2adb-414f-8726-e6060ddaa56a","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T16:48:04.617969Z","iopub.execute_input":"2024-12-11T16:48:04.618384Z","iopub.status.idle":"2024-12-11T16:48:04.640699Z","shell.execute_reply.started":"2024-12-11T16:48:04.618350Z","shell.execute_reply":"2024-12-11T16:48:04.639848Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"LongformerForSequenceClassification(\n  (longformer): LongformerModel(\n    (embeddings): LongformerEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n    )\n    (encoder): LongformerEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x LongformerLayer(\n          (attention): LongformerAttention(\n            (self): LongformerSelfAttention(\n              (query): lora.Linear(\n                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=768, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=768, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (key): lora.Linear(\n                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=768, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=768, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (value): lora.Linear(\n                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=768, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=768, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (query_global): lora.Linear(\n                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=768, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=768, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (key_global): lora.Linear(\n                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=768, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=768, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (value_global): lora.Linear(\n                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=768, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=768, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (output): LongformerSelfOutput(\n              (dense): lora.Linear(\n                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=768, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=768, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LongformerIntermediate(\n            (dense): lora.Linear(\n              (base_layer): Linear(in_features=768, out_features=3072, bias=True)\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.1, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=768, out_features=16, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=16, out_features=3072, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n              (lora_magnitude_vector): ModuleDict()\n            )\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LongformerOutput(\n            (dense): lora.Linear(\n              (base_layer): Linear(in_features=3072, out_features=768, bias=True)\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.1, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=3072, out_features=16, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=16, out_features=768, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n              (lora_magnitude_vector): ModuleDict()\n            )\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): ModulesToSaveWrapper(\n    (original_module): LongformerClassificationHead(\n      (dense): lora.Linear(\n        (base_layer): Linear(in_features=768, out_features=768, bias=True)\n        (lora_dropout): ModuleDict(\n          (default): Dropout(p=0.1, inplace=False)\n        )\n        (lora_A): ModuleDict(\n          (default): Linear(in_features=768, out_features=16, bias=False)\n        )\n        (lora_B): ModuleDict(\n          (default): Linear(in_features=16, out_features=768, bias=False)\n        )\n        (lora_embedding_A): ParameterDict()\n        (lora_embedding_B): ParameterDict()\n        (lora_magnitude_vector): ModuleDict()\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n      (out_proj): Linear(in_features=768, out_features=2, bias=True)\n    )\n    (modules_to_save): ModuleDict(\n      (default): LongformerClassificationHead(\n        (dense): lora.Linear(\n          (base_layer): Linear(in_features=768, out_features=768, bias=True)\n          (lora_dropout): ModuleDict(\n            (default): Dropout(p=0.1, inplace=False)\n          )\n          (lora_A): ModuleDict(\n            (default): Linear(in_features=768, out_features=16, bias=False)\n          )\n          (lora_B): ModuleDict(\n            (default): Linear(in_features=16, out_features=768, bias=False)\n          )\n          (lora_embedding_A): ParameterDict()\n          (lora_embedding_B): ParameterDict()\n          (lora_magnitude_vector): ModuleDict()\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n        (out_proj): Linear(in_features=768, out_features=2, bias=True)\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"### Sanity-check the reward model\n\nLet's check how our reward model performs.\n\n__Your task__ is to measure how often does your reward model can rank a pair of (chosen and rejected) reviews correctly. Please measure this separately for train data (`imdb`) and a separate test set loaded below.","metadata":{"id":"wZIaS-gRo8yc"}},{"cell_type":"code","source":"\nfor sample_index in 45, 16000:\n  print('TEXT:', imdb[sample_index]['text'])\n  inputs = reward_tokenizer(imdb[sample_index]['text'], truncation=True, return_tensors='pt').to(device)\n  with torch.no_grad():\n    reward = reward_model(**inputs).logits[0, 0].item()\n    print(\"REWARD:\", reward)\n  print('LABEL:', imdb[sample_index]['label'])\n  print()\n\n# note: your reward model may produce different absolute rewards.\n# This is fine as long as the rewards are ordered correctly (most of the time)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IeQ108nOZ7nO","outputId":"576a2031-f1fc-49cc-b470-c9fd4983ffbf","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T16:48:20.318237Z","iopub.execute_input":"2024-12-11T16:48:20.318963Z","iopub.status.idle":"2024-12-11T16:48:20.477265Z","shell.execute_reply.started":"2024-12-11T16:48:20.318933Z","shell.execute_reply":"2024-12-11T16:48:20.476348Z"}},"outputs":[{"name":"stdout","text":"TEXT: This movie sucked. It really was a waste of my life. The acting was atrocious, the plot completely implausible. Long, long story short, these people get \"terrorized\" by this pathetic \"crazed killer\", but completely fail to fight back in any manner. And this is after they take a raft on a camping trip, with no gear, and show up at a campsite that is already assembled and completely stocked with food and clothes and the daughters headphones. Additionally, after their boat goes missing, they panic that they're stuck in the woods, but then the daughters boyfriend just shows up and they apparently never consider that they could just hike out of the woods like he did to get to them. Like I said, this movie sucks. A complete joke. Don't let your girlfriend talk you into watching it.\nREWARD: 5.080700397491455\nLABEL: 0\n\nTEXT: Good: Engaging cinematic firefights, great presentation, vehicles are actually fun to drive, fairly appealing multiplayer, faithful to the movie, and the list goes on.<br /><br />Bad: Main missions are a bit short.<br /><br />This game defines what a \"good\" third person shooter(not necessarily a spy-game) is. Great firefights carry on the story and make you want to complete EVERY single mission through, and unlock all the genuine bonuses the game has to offer. The hype this game had, was lived up to, and I personally think you should buy it, and hook up with a couple of friends and play this one. Loads of fun. <br /><br />The sound in this game, is a rip-roaring achievement from a few previous bond games, and firing a weapon, really feels like you're firing a weapon. It ties in with the aspect that you are a deadly and ruthless spy.<br /><br />All in all, this game makes you excited and satisfied after you make it through, and some multiplayer that can compete with the standards of the crafty James Bond \"Nightfire\" game for gamecube.\nREWARD: -4.051922798156738\nLABEL: 1\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"First of all, let's implement `compute_reward` function. Note that we use plaintext reviews because main model uses a different tokenizer from the reward model.","metadata":{"id":"GITb0RF_FVUH"}},{"cell_type":"code","source":"from torch import Tensor, no_grad\n\ndef compute_reward(reward_model, reward_tokenizer, texts: list[str], device='cpu') -> Tensor:\n    \"\"\"\n    Compute the reward scores for a list of texts using a specified reward model and tokenizer.\n\n    Parameters:\n    reward_model: The model used to compute the reward scores\n    reward_tokenizer: The tokenizer for reward_model\n    texts (list[str]): A list of text strings for which the reward scores are to be computed.\n    device (str, optional): The device on which the computation should be performed. Default is 'cpu'.\n\n    Returns:\n    torch.Tensor: A tensor containing the reward scores for each input text. The scores are extracted\n                  from the logits of the reward model.\n\n    Example:\n    >>> compute_reward(my_reward_model, my_reward_tokenizer, [\"text1\", \"text2\"])\n    tensor([ 5.1836, -4.8438], device='cpu')\n    \"\"\"\n\n    # <YOUR CODE HERE>\n    encodings = reward_tokenizer(texts, return_tensors='pt', padding=True).to(device)\n\n    with no_grad():\n        # <YOUR CODE HERE>\n        reward_scores = reward_model(**encodings).logits[:, 0]\n    \n    return reward_scores","metadata":{"id":"Y6eFIM_wlJrn","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T16:54:51.284552Z","iopub.execute_input":"2024-12-11T16:54:51.285453Z","iopub.status.idle":"2024-12-11T16:54:51.293356Z","shell.execute_reply.started":"2024-12-11T16:54:51.285395Z","shell.execute_reply":"2024-12-11T16:54:51.292116Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"rewards = compute_reward(reward_model, reward_tokenizer, [imdb[45]['text'], imdb[16000]['text']], device=device)\nprint(rewards)\nassert rewards[0] > rewards[1]\nassert rewards[0] > 0\nassert rewards[1] < 0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BhJ4WFjBhOMA","outputId":"8a5c9990-dc7e-421b-af04-6871a3efa2fe","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T16:54:54.610389Z","iopub.execute_input":"2024-12-11T16:54:54.610746Z","iopub.status.idle":"2024-12-11T16:54:54.782660Z","shell.execute_reply.started":"2024-12-11T16:54:54.610716Z","shell.execute_reply":"2024-12-11T16:54:54.781865Z"}},"outputs":[{"name":"stdout","text":"tensor([ 5.0807, -4.0519], device='cuda:0')\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\ndef eval_reward_model(reward_model, reward_tokenizer, test_dataset, target_label, device='cpu'):\n    \"\"\"\n    Evaluate the performance of a reward model by comparing reward scores for chosen and rejected reviews. \n\n    This function selects reviews from a test dataset based on a target label and evaluates the reward model's\n    ability to assign higher scores to chosen reviews compared to rejected ones. The evaluation is performed\n    in batches for efficiency.\n    Note that reward scores are compared on corresponding chosen and rejected reviews: \n        chosen_reviews[0] vs rejected_reviews[0], \n        chosen_reviews[1] vs rejected_reviews[1],\n        etc.\n\n    Parameters:\n    reward_model: The model used to compute the reward scores\n    reward_tokenizer: The tokenizer for reward_model\n    tes_dataset: test Dataset\n    target_label (0 or 1): The label used to select chosen reviews. Reviews with this label are considered chosen,\n                  while others are considered rejected.\n    device (str, optional): The device on which the computation should be performed. Default is 'cpu'.\n\n    Returns:\n    float: The accuracy of the reward model, calculated as the proportion of times the model assigns a higher\n           reward score to the chosen review compared to the rejected review.\n\n    Example:\n    >>> accuracy = eval_reward_model(my_reward_model, my_reward_tokenizer, test_data, target_label=1)\n    >>> print(f\"Model accuracy: {accuracy:.2%}\")\n    \"\"\"\n    # <YOUR CODE HERE>\n\n    chosen_reviews = [sample['text'] for sample in test_dataset if sample['label'] == target_label]\n    rejected_reviews = [sample['text'] for sample in test_dataset if sample['label'] != target_label]\n\n    assert len(chosen_reviews) == len(rejected_reviews)\n\n    # <YOUR CODE HERE>\n    correct_count = 0 \n    total_count = len(chosen_reviews)\n\n    batch_size = 8\n    for i in tqdm(range(0, total_count, batch_size), desc=\"Evaluating\"):\n        batch_chosen = chosen_reviews[i: i + batch_size]\n        batch_rejected = rejected_reviews[i: i + batch_size]\n\n        reward_chosen = compute_reward(reward_model, reward_tokenizer, batch_chosen, device=device)\n        reward_rejected = compute_reward(reward_model, reward_tokenizer, batch_rejected, device=device)\n        \n        correct_count += (reward_chosen > reward_rejected).sum().item()\n    \n    accuracy = correct_count / total_count\n    \n    return accuracy","metadata":{"id":"wzxa8k37mPS7","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T16:55:14.648043Z","iopub.execute_input":"2024-12-11T16:55:14.648820Z","iopub.status.idle":"2024-12-11T16:55:14.656221Z","shell.execute_reply.started":"2024-12-11T16:55:14.648784Z","shell.execute_reply":"2024-12-11T16:55:14.655276Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"imdb_test = datasets.load_dataset(\"imdb\", split='test')\n\ntest_accuracy = eval_reward_model(\n    reward_model,\n    reward_tokenizer,\n    imdb_test,\n    target_label=TARGET_LABEL,\n    device=device,\n)\n\nprint('test accuracy: {}'.format(test_accuracy))\nassert test_accuracy > 0.94","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["f70928bb66b24415ad03f49633775e64","11b0022cfbaa488db010a2f22a17b7cc","015524419b8a4a21bec9ca2462a1bdae","351c280e3e0644e08478893632ca878c","9123a18088fb454d915218f37f72a790","3fd865cd993242cdad016d9794a75f6e","f986e194b4944ead94190797f28015fe","c3bb5755a3f44b519c4203430846e3d2","62e010f8c15d402aba1e045de8e8a50b","579edffceded4184ab0009cef5e8d87e","83eac7e3fc554ea09c2a41452cad2cde"]},"id":"1OvHokADapTh","outputId":"9921947c-07c7-4821-a706-f6f945d4f08c","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T16:55:19.284598Z","iopub.execute_input":"2024-12-11T16:55:19.285070Z","iopub.status.idle":"2024-12-11T17:30:45.302041Z","shell.execute_reply.started":"2024-12-11T16:55:19.285031Z","shell.execute_reply":"2024-12-11T17:30:45.300929Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1563 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03ca750a173c4cd6b97a98376a3c4b9c"}},"metadata":{}},{"name":"stdout","text":"test accuracy: 0.98432\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"### Reward-guided generation (1 point)\n\nIf you did everything right, by now you should have a decent reward model. Before we use it for reinforcement learning, let's see if we can align model samples without any training.\n\nTo do so, you can use reward-guided inference: __generate N=16 samples, then select the one with the highest reward__ (according to your reward model).\n\nFor this problem, it's on you to demonstrate whether or not your code works. Find at least 5 neutral prompts such as \"This movie is\" (...), generate samples, rank them based on reward and show which samples get the highest reward.\n\nNote: it is faster to generate samples in parallel, rather than sequentially, as follows:\n\n\n","metadata":{"id":"NHCWHMyRw2-k"}},{"cell_type":"code","source":"inputs = main_tokenizer([\"It was\"] * 5, return_tensors='pt').to(device)\nfor candidate in main_model.generate(**inputs, max_new_tokens=50, do_sample=True):\n  print(\"Sample:\", main_tokenizer.decode(candidate.flatten().cpu().numpy().tolist()))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8BRsyb2cq5dR","outputId":"af7df016-7656-47b9-9d7b-72bebb62c4b2","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:32:35.007531Z","iopub.execute_input":"2024-12-11T17:32:35.008284Z","iopub.status.idle":"2024-12-11T17:32:35.465863Z","shell.execute_reply.started":"2024-12-11T17:32:35.008249Z","shell.execute_reply":"2024-12-11T17:32:35.464905Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample: It was a great movie to watch. Some of the dialogue I read there (including the \"what if, you came on this island and killed so bad\" part of the movie) is very familiar I guess to any real islander. Plus, what I\nSample: It was really a waste of time to try this film. I really wouldn't recommend this film to anyone except friends. Anyone who's at the theatre at school, or the theatre and wants to see this film then this is the movie for you.<br /\nSample: It was really funny. What a great film. I was laughing through it. I know I was watching the ending and all of the tension in this movie was lost in all of what went on in that theater. In the end it turned out that if you\nSample: It was actually very disappointing to see so many of his performances on TV. There were many, many errors in the production design and editing.<br /><br />To summarize, I felt that the show's plot was a very poor attempt at establishing a premise\nSample: It was very difficult to play without becoming a man of his word. As a rule, I found myself watching most of the characters only once during the movie. Although there were moments in the movie where it had become clear, the rest was never a serious plot\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"def generate_with_reward_guidance(\n        main_model, main_tokenizer,\n        reward_model, reward_tokenizer,\n        N=16,\n        device='cpu',\n    ):\n    \"\"\"\n    Generate text samples using a main model and select the best sample based on a reward model's guidance.\n\n    This function generates multiple text samples from a main model, evaluates each sample using a reward model,\n    and returns the sample with the highest reward score. The process is guided by the reward model to select\n    the most desirable output.\n\n    Parameters:\n    main_model: The language model used to generate text samples.\n    main_tokenizer: The tokenizer for main_model\n    reward_model: The model used to compute reward scores for the generated samples.\n    reward_tokenizer: The tokenizer for reward_model\n    N (int, optional): The number of text samples to generate. Default is 16.\n    device (str, optional): The device on which the computation should be performed. Default is 'cpu'.\n\n    Returns:\n    str: The generated text sample with the highest reward score.\n    \"\"\"\n\n    # <YOUR CODE HERE>\n    if main_tokenizer.pad_token is None:\n        main_tokenizer.pad_token = main_tokenizer.eos_token\n\n    input_text = \"The movie was\"\n    inputs = main_tokenizer(\n        input_text, \n        return_tensors='pt', \n        padding=True, \n        truncation=True, \n        max_length=512\n    ).to(device)\n\n    generated_texts = []\n    for _ in range(N):\n        with no_grad():\n            output = main_model.generate(\n                input_ids=inputs['input_ids'],\n                attention_mask=inputs['attention_mask'],\n                max_length=50,\n                num_return_sequences=1,\n                do_sample=True,\n                top_k=50,\n                pad_token_id=main_tokenizer.pad_token_id\n            )\n        decoded_text = main_tokenizer.decode(output[0], skip_special_tokens=True)\n        generated_texts.append(decoded_text)\n\n    reward_scores = compute_reward(reward_model, reward_tokenizer, generated_texts, device=device)\n\n    best_index = reward_scores.argmax().item()\n    best_text = generated_texts[best_index]\n    \n    return best_text","metadata":{"id":"r08F4lz7yxE1","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:32:42.302736Z","iopub.execute_input":"2024-12-11T17:32:42.303556Z","iopub.status.idle":"2024-12-11T17:32:42.310537Z","shell.execute_reply.started":"2024-12-11T17:32:42.303522Z","shell.execute_reply":"2024-12-11T17:32:42.309571Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"generate_with_reward_guidance(\n    main_model, main_tokenizer,\n    reward_model, reward_tokenizer,\n    device=device,\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":71},"id":"cI5kuJ-Gb6Pn","outputId":"b1ef6774-5a12-43d1-aa5f-50efedd96b8c","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:32:50.868255Z","iopub.execute_input":"2024-12-11T17:32:50.868947Z","iopub.status.idle":"2024-12-11T17:32:57.766681Z","shell.execute_reply.started":"2024-12-11T17:32:50.868915Z","shell.execute_reply":"2024-12-11T17:32:57.765701Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"\"The movie was poorly acted to boot, and didn't keep pace with the rest of the cast. The pacing was almost comical, but had nothing more to do than make the actors laugh at themselves--and that is fine if people in big film\""},"metadata":{}}],"execution_count":29},{"cell_type":"markdown","source":"# Stage 2: fine-tune the main model with RL\n\n\nNow, we will optimize GPT2 to produce negative IMDB movie reviews using the reward model you trained above.\n\nUnlike supervised fine-tuning, RL allows model to generate it's own sentences on each training step. Then, it calculates the reward of those specific sentences, and finally, updates the model to increase the probability of sentences with high reward.\n\nThus, each RLHF consists of three stages: __Rollout__, __Evaluation__ and __Update__\n\n<div style=\"text-align: center\">\n<img src='https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/gpt2_bert_training.png' width='600'>\n\nThe update stage depends on the specific RL algorithm. We'll be using Proximal Policy Optimization, or [PPO](https://arxiv.org/abs/1707.06347), similarly to what was used for InstructGPT.\n\nBefore we run those 3 stages, however, we need to create a dataset of \"queries\" - partial reviews in our case.","metadata":{"id":"8NjQ40BRoH5f"}},{"cell_type":"code","source":"# Note: this code is specific to IMDB; you will need to re-write it for other tasks\nimdb_for_rlhf = imdb.filter(lambda row: len(row['text']) > 200, batched=False)\nimdb_for_rlhf = imdb_for_rlhf.remove_columns(['label'])\nsample_length = trl.core.LengthSampler(2, 8)  # use the first 2-8 tokens as query\n\ndef select_query_and_tokenize(sample):\n    query_ids = main_tokenizer.encode(sample[\"text\"])[: sample_length()]\n    sample[\"query\"] = main_tokenizer.decode(query_ids)  # query is the only required column\n    sample[\"input_ids\"] = query_ids  # to avoid re-tokenizing later\n    return sample  # we do not need the rest - it will be generated by the model\n\nimdb_for_rlhf = imdb_for_rlhf.map(select_query_and_tokenize, batched=False)\nimdb_for_rlhf.set_format(type=\"torch\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":119,"referenced_widgets":["14bab8c0ec8f4b7ea41d4e74a31d280b","c6d1254f73fc441897fb289661579fcc","4cdf531ea3f545ffa6e0d0f68389f653","a36ee9303657431093a8b33d713e9bd8","daad181cf83044a5bc285f7925176464","5482e18aa0564a50a7599875aa7e053f","4ddb8f654c5a45e38c588cac213d0eb8","f67d8a3f95074b01853851dd5b41cfa6","b4b6c04b33e44613a3736505a4899d71","f49a71e7d53342809dc887e254aa9d9f","c52d8a32faa54cd9b7541441d3551ded","5470a457a3e2466789ab2f3ca3289daa","afb680bcb0fc4f4a94f04b5c086f77c6","e76aa419c1864ab7906f58c09b53efb8","0970efe0166f40b4a4355f474bfe182a","64858c8a884d4e0792e0f6894015ead5","a0134acdd9764db08cdb145a3b816eb3","b07ef37f928646c7807471849cf6f71a","5dab60bfa0f54cadbc163b4331732185","a35d7bf116124060b525d12cd119ac5b","305a6c274faa46beb46e8a2eff32d0fe","5eec4a26a59c4b108bf576f21cad07f0"]},"id":"jm5IUrer0xd_","outputId":"2acab8d0-b1c1-4742-ec9b-95f8647d0526","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:33:06.967223Z","iopub.execute_input":"2024-12-11T17:33:06.967555Z","iopub.status.idle":"2024-12-11T17:33:33.105190Z","shell.execute_reply.started":"2024-12-11T17:33:06.967530Z","shell.execute_reply":"2024-12-11T17:33:33.104335Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5d2ebd6073948509defa338e68b66e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/24895 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6854ffd33f4d4ed984afeb9fec2edee8"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"Finally, we move to RL training. In this tutorial, we'll train LoRA adapters and not the full model.","metadata":{"id":"U3buACYV4QLJ"}},{"cell_type":"code","source":"import peft\npeft_config = peft.LoraConfig(\n    task_type=peft.TaskType.CAUSAL_LM, r=32, lora_alpha=32, lora_dropout=0.0, inference_mode=False\n)\n\n# reload main model as AutoModelForCausalLMWithValueHead - with an extra head needed for PPO\nmain_tokenizer = transformers.AutoTokenizer.from_pretrained(\"lvwerra/gpt2-imdb\")\nmain_tokenizer.pad_token = main_tokenizer.eos_token\n\nmain_model = trl.AutoModelForCausalLMWithValueHead.from_pretrained(\"lvwerra/gpt2-imdb\", device_map=device)\nmain_model = peft.get_peft_model(main_model, peft_config, adapter_name='default')\nmain_model.print_trainable_parameters()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nar1yXgl4KQa","outputId":"6fcd294c-3151-4494-a62f-14e4989e4259","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:34:14.423267Z","iopub.execute_input":"2024-12-11T17:34:14.424074Z","iopub.status.idle":"2024-12-11T17:34:17.555635Z","shell.execute_reply.started":"2024-12-11T17:34:14.424040Z","shell.execute_reply":"2024-12-11T17:34:17.554675Z"}},"outputs":[{"name":"stdout","text":"trainable params: 1,179,648 || all params: 125,620,225 || trainable%: 0.9391\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"Same as before, trl has a special type of trainer that minimize PPO-specific pseudo-loss. You can read more on this trainer [here](https://huggingface.co/docs/trl/main/en/ppo_trainer).","metadata":{"id":"qIQK5bcpCPZ6"}},{"cell_type":"code","source":"reward_model.save_pretrained('./my_model')\nreward_tokenizer.save_pretrained('./my_model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T18:42:43.543656Z","iopub.execute_input":"2024-12-11T18:42:43.544500Z","iopub.status.idle":"2024-12-11T18:42:44.970867Z","shell.execute_reply.started":"2024-12-11T18:42:43.544463Z","shell.execute_reply":"2024-12-11T18:42:44.969915Z"}},"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"('./my_model/tokenizer_config.json',\n './my_model/special_tokens_map.json',\n './my_model/vocab.json',\n './my_model/merges.txt',\n './my_model/added_tokens.json',\n './my_model/tokenizer.json')"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"training_args = trl.PPOConfig(\n    model_name=main_model.config._name_or_path,\n    gradient_accumulation_steps=1,\n    learning_rate=1.41e-5,\n    batch_size=64,\n    ppo_epochs=4,                 # PPO performs this many updates per training batch\n)\n\nppo_trainer = trl.PPOTrainer(\n    training_args, model=main_model.model, tokenizer=main_tokenizer,\n    dataset=imdb_for_rlhf, data_collator=lambda data: dict((key, [d[key] for d in data]) for key in data[0])\n)  # note: we pass main_model.model because PPOTrainer checks for one of several supported model types ...\n# ... main_model.model is a model with adapters, which is supported. main_model itself is a wrapper that is not supported","metadata":{"id":"m4gvjdKbmeJf","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T18:22:39.338899Z","iopub.execute_input":"2024-12-11T18:22:39.339354Z","iopub.status.idle":"2024-12-11T18:22:39.389392Z","shell.execute_reply.started":"2024-12-11T18:22:39.339317Z","shell.execute_reply":"2024-12-11T18:22:39.387956Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mtrl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPPOConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.41e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mppo_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# PPO performs this many updates per training batch\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m ppo_trainer \u001b[38;5;241m=\u001b[39m trl\u001b[38;5;241m.\u001b[39mPPOTrainer(\n\u001b[1;32m     10\u001b[0m     training_args, model\u001b[38;5;241m=\u001b[39mmain_model\u001b[38;5;241m.\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mmain_tokenizer,\n\u001b[1;32m     11\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mimdb_for_rlhf, data_collator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m data: \u001b[38;5;28mdict\u001b[39m((key, [d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     12\u001b[0m )  \u001b[38;5;66;03m# note: we pass main_model.model because PPOTrainer checks for one of several supported model types ...\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ... main_model.model is a model with adapters, which is supported. main_model itself is a wrapper that is not supported\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: PPOConfig.__init__() got an unexpected keyword argument 'model_name'"],"ename":"TypeError","evalue":"PPOConfig.__init__() got an unexpected keyword argument 'model_name'","output_type":"error"}],"execution_count":54},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\nmax_steps = 50   # can be insufficient for some tasks - watch your learning curves\ngeneration_kwargs = dict(\n    min_length=-1, max_new_tokens=128, do_sample=True, top_k=0, top_p=1.0, pad_token_id=main_tokenizer.eos_token_id)\n#                                  ^-- task-specific parameter!\n\naverage_reward = 0\ngamma = 0.7\n\nwith tqdm(enumerate(ppo_trainer.dataloader), total=max_steps) as progressbar:\n  # note: ppo_trainer.dataloader is just a regular dataloader of queries, no RL-specific magic :)\n  for epoch, batch in progressbar:\n    if epoch >= max_steps:\n        break\n\n    # Rollout stage: generate continuations from batch queries using main_model\n    response_tensors = ppo_trainer.generate(batch['input_ids'], **generation_kwargs)\n    # ^-- list of tensors of token ids from main model tokenizer\n\n    # de-tokenize responses to strings (since reward model uses a different tokenizer)\n    batch[\"response\"] = [main_tokenizer.decode(response.squeeze()) for response in response_tensors]\n    # note: response_tensors already contain query tokens, so we don't need to add queries manually.\n    # This may not be true for other tasks: check this manually by viewing batch[\"response\"] and batch[\"query\"]\n\n\n    # Evaluation stage - rewards for batch['response']\n    rewards = compute_reward(reward_model, reward_tokenizer, batch[\"response\"], device=device) # <YOUR CODE HERE>\n\n    # Update stage\n    stats = ppo_trainer.step(batch['input_ids'], response_tensors, list(rewards.split(1)))\n    stats['rewards/mean'] = rewards.mean().item() # <YOUR CODE HERE> - compute mean rewards for batch\n    average_reward = gamma * average_reward + (1 - gamma) * stats['rewards/mean']\n\n    print(\"-\" * 30, 'STEP', epoch, '-' * 30)\n    print(f'rewards/mean:\\t{stats[\"rewards/mean\"]:.9f}\\t<---- average reward over this batch (higher=better, noisy)')\n    print(f'rewards/moving_avg:\\t{average_reward:.9f}\\t<---- moving average reward (higher=better, less noisy)')\n    print(f'ppo/returns/mean:\\t{stats[\"ppo/returns/mean\"]:.9f}\\t<---- model-estimated average discounted reward')\n    print(f'objective/kl:\\t{stats[\"objective/kl\"]:.9f}\\t<---- how far we are from the original model (regularizer)')\n    print()\n\n    ppo_trainer.log_stats(stats, batch, list(rewards.split(1)))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["90b8fdb8b9fb438f84c4f390683b0f3f","af3a05a0c4ca49a385776d6c42427f8a","ce8dea39d6b44be18e8f94392f94e7ce","7b5922a62de94ee29cb6eefc9193190d","0daa5834f1834dea9d5e40e6a3c4b376","11f6a941d42f4692a53f94f36aca62d5","6d693232f3c048ad944a47ee5742187c","d833f6bf46c04b64acd1c04165c8f813","0033ac842aab47e48f35151855112979","60cbfb4df6da47d4b8b9624047b91cf2","73db6e451d7c401d8b341c4352809e20"]},"id":"eYr-w666-QfK","outputId":"9dc19617-96cc-4ecc-d49f-dc8326a363e0","scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/50 [00:00<?, ?it/s]\n"]},{"ename":"AssertionError","evalue":"Torch not compiled with CUDA enabled","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[58], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Rollout stage: generate continuations from batch queries using main_model\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m response_tensors \u001b[38;5;241m=\u001b[39m ppo_trainer\u001b[38;5;241m.\u001b[39mgenerate(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_kwargs)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# ^-- list of tensors of token ids from main model tokenizer\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# de-tokenize responses to strings (since reward model uses a different tokenizer)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [main_tokenizer\u001b[38;5;241m.\u001b[39mdecode(response\u001b[38;5;241m.\u001b[39msqueeze()) \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m response_tensors]\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\trl\\trainer\\ppo_trainer.py:459\u001b[0m, in \u001b[0;36mPPOTrainer.generate\u001b[1;34m(self, query_tensor, length_sampler, batch_size, return_prompt, generate_ref_response, **generation_kwargs)\u001b[0m\n\u001b[0;32m    457\u001b[0m     ref_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_peft_model \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mref_model\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(query_tensor, List):\n\u001b[1;32m--> 459\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_batched(\n\u001b[0;32m    460\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m    461\u001b[0m         query_tensor,\n\u001b[0;32m    462\u001b[0m         length_sampler\u001b[38;5;241m=\u001b[39mlength_sampler,\n\u001b[0;32m    463\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m    464\u001b[0m         return_prompt\u001b[38;5;241m=\u001b[39mreturn_prompt,\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_kwargs,\n\u001b[0;32m    466\u001b[0m     )\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generate_ref_response:\n\u001b[0;32m    468\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptional_peft_ctx():\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\trl\\trainer\\ppo_trainer.py:533\u001b[0m, in \u001b[0;36mPPOTrainer._generate_batched\u001b[1;34m(self, model, query_tensors, length_sampler, batch_size, return_prompt, pad_to_multiple_of, remove_padding, **generation_kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m batch_mask \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mones_like(element) \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m    531\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_mask}\n\u001b[1;32m--> 533\u001b[0m padded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    541\u001b[0m generations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39munwrap_model(model)\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpadded_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_kwargs)\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m generation, mask \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(generations, padded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:777\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m--> 777\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    779\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:777\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m--> 777\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    779\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\cuda\\__init__.py:310\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m     )\n","\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"]}],"execution_count":null},{"cell_type":"code","source":"assert average_reward > 2","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":144},"id":"Xoq0vpCxx5IZ","outputId":"a01352f3-5b33-4a1a-9391-09abc4003868"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"And now test your PPO model:","metadata":{}},{"cell_type":"code","source":"inputs = [main_tokenizer.encode(\"The movie was\", return_tensors='pt').to(device)[0] for i in range(5)]\n\nresponse_tensors = ppo_trainer.generate(inputs, **generation_kwargs)\nbatch[\"response\"] = [main_tokenizer.decode(response.squeeze()) for response in response_tensors]\nfor sample in batch[\"response\"]:\n    print('Sample: {}'.format(sample))","metadata":{"id":"_RV1n2vgiZP_"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sample: The movie was written by Omar Bakari of Ir Daily Show in Internet novels works, najd-as theor death, I didn't know when he made this thing, but it wasn't bad. def shines for Great F****** Good for He subordinates.<br /><br />This IS BEST AROUND! I was not expecting it to be a science movie it an A. failed to have a schtick to make it academically, it sucked and failed to do the admirable job with he job.<|endoftext|>\n","Sample: The movie was really awful. I aren't referring to the tiradist antics with what adjects. The whole plot summary bombing and launching cost vulgomer that is Bochowski Centerll was completely boring, complete and completely pointless. The greatest waste of David Cameron movie theater, and the man with the most unstable personality I have ever seen in a movie.<|endoftext|>\n","Sample: The movie was kind of boring...Some drawings looks like graphic shopgirls and they take some ends wrong in their new ways; in the beginning they do the original drawings, because they're too late. After the movie is over they have tattoos, pokes, etc... And some of the scenes are just so terrible... but the director is basically thrown in the trash bins for the reason that the visiting photo guy is all unloaded after the right time. It's like they try to kill the joyful animals these drawings for. But they wouldn't do that.<|endoftext|>\n","Sample: The movie was galloping painfully slow in the ending, which burns them wounded with hideous experience. The first ending, predictable? I heard Random House would end with a direct insult to The Matrix. Could it be that same thing going on? You had to learn other things than that.<|endoftext|>\n","Sample: The movie was all directed by Killer Mike. Not all of the little refugees from that movie. Horror claimed in person.<br /><br />This movie is bad bad bad bad BAD BAD. This's action-less rabbit trips and are psycho-flambulantly hurting. MAD!<|endoftext|>\n"]}],"execution_count":null}]}